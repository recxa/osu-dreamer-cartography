{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Cartography: Pipeline Walkthrough\n",
    "\n",
    "This notebook walks through the core analysis pipeline for our cross-mapper beatmap study. The goal: by comparing how **different mappers** annotate the **same songs**, we can identify which aspects of rhythm representation are **perceptually universal** (all mappers agree) vs. **stylistic** (mapper-specific).\n",
    "\n",
    "The pipeline has two tracks:\n",
    "- **Track A** — Compare mappers in a 9-dimensional interpretable space (onset, combo, slider, etc.)\n",
    "- **Track B** — Compare mappers in a 32-dimensional VAE latent space (learned representation)\n",
    "- **Synthesis** — Correlate the two spaces to understand what the VAE learns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: What are we measuring?\n",
    "\n",
    "In osu!, a **beatmap** is a human-authored annotation of a song — placing hit objects in time and space to match perceived rhythm. When multiple mappers independently create beatmaps for the same song, their maps reflect a mix of:\n",
    "\n",
    "1. **Perceptual agreement** — aspects of rhythm that all humans hear the same way (e.g., a loud drum hit → everyone places an onset there)\n",
    "2. **Stylistic choice** — artistic decisions that vary by mapper (e.g., cursor movement patterns, difficulty preferences)\n",
    "\n",
    "Our metric is **Pearson correlation (r)** between pairs of mappers on the same song:\n",
    "\n",
    "| Range | Interpretation |\n",
    "|-------|---------------|\n",
    "| r > 0.5 | **Strong agreement** — likely perceptual |\n",
    "| r = 0.3–0.5 | **Moderate agreement** |\n",
    "| r = 0.1–0.3 | **Weak agreement** — mixed |\n",
    "| r < 0.1 | **No agreement** — stylistic / mapper-specific |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 9 Interpretable Dimensions\n",
    "\n",
    "Each beatmap is encoded as a 9-channel temporal signal (one value per audio frame):\n",
    "\n",
    "| # | Dimension | What it captures |\n",
    "|---|-----------|------------------|\n",
    "| 0 | **ONSET** | Is there a hit object at this moment? (binary) |\n",
    "| 1 | **COMBO** | New combo flag — mapper's phrasing of the rhythm |\n",
    "| 2 | **SLIDE** | Is a slider active? (sustained hits) |\n",
    "| 3 | **SUSTAIN** | Slider body / hold duration |\n",
    "| 4 | **WHISTLE** | Whistle hitsound (high-pitched accent) |\n",
    "| 5 | **FINISH** | Finish hitsound (cymbal crash) |\n",
    "| 6 | **CLAP** | Clap hitsound (snare-like accent) |\n",
    "| 7 | **X** | Horizontal cursor position (0–512) |\n",
    "| 8 | **Y** | Vertical cursor position (0–384) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Find Multi-Mapper Songs\n",
    "\n",
    "We scan a folder of `.osz` beatmap archives and group them by song (matching on title + artist). We keep only songs that have been mapped by **2 or more different mappers** — these are our natural experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, re, json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def scan_osz_folder(dataset_dir):\n",
    "    \"\"\"Read metadata from each .osz file and group by song.\"\"\"\n",
    "    songs = defaultdict(list)\n",
    "    \n",
    "    for osz_path in Path(dataset_dir).glob(\"*.osz\"):\n",
    "        with zipfile.ZipFile(osz_path) as z:\n",
    "            # Each .osz contains one or more .osu files (difficulty versions)\n",
    "            osu_files = [n for n in z.namelist() if n.endswith(\".osu\")]\n",
    "            content = z.read(osu_files[0]).decode(\"utf-8\", errors=\"replace\")\n",
    "            \n",
    "            # Extract key fields from the .osu text format\n",
    "            title = re.search(r\"^Title:(.+)$\", content, re.MULTILINE).group(1).strip()\n",
    "            artist = re.search(r\"^Artist:(.+)$\", content, re.MULTILINE).group(1).strip()\n",
    "            creator = re.search(r\"^Creator:(.+)$\", content, re.MULTILINE).group(1).strip()\n",
    "            \n",
    "            songs[(title.lower(), artist.lower())].append({\n",
    "                \"title\": title, \"artist\": artist, \"creator\": creator,\n",
    "                \"filename\": osz_path.name,\n",
    "            })\n",
    "    \n",
    "    # Keep only songs with 2+ different mappers\n",
    "    multi_mapper = []\n",
    "    for (title, artist), entries in songs.items():\n",
    "        mappers = set(e[\"creator\"] for e in entries)\n",
    "        if len(mappers) >= 2:\n",
    "            multi_mapper.append({\n",
    "                \"title\": entries[0][\"title\"],\n",
    "                \"artist\": entries[0][\"artist\"],\n",
    "                \"num_mappers\": len(mappers),\n",
    "                \"beatmapsets\": entries,\n",
    "            })\n",
    "    \n",
    "    return multi_mapper\n",
    "\n",
    "# Example: multi_mapper = scan_osz_folder(\"/path/to/osz/files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build the Beatmap Registry\n",
    "\n",
    "For each multi-mapper song, we extract the `.osu` files and parse their metadata. We filter to **Mode 0** (osu!standard — the classic click-circle mode) and select one **representative** beatmap per mapper per song (the highest difficulty, measured by Overall Difficulty).\n",
    "\n",
    "This representative selection ensures we're comparing each mapper's \"best effort\" on the same song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_osu_metadata(osu_path):\n",
    "    \"\"\"Parse an .osu file for difficulty settings and metadata.\"\"\"\n",
    "    content = Path(osu_path).read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "    def get(name):\n",
    "        m = re.search(rf\"^{name}:\\s*(.+)$\", content, re.MULTILINE)\n",
    "        return m.group(1).strip() if m else None\n",
    "    \n",
    "    # Only keep Mode 0 (osu!standard)\n",
    "    if get(\"Mode\") != \"0\":\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"title\": get(\"Title\"),\n",
    "        \"artist\": get(\"Artist\"),\n",
    "        \"creator\": get(\"Creator\"),\n",
    "        \"version\": get(\"Version\"),        # difficulty name, e.g. \"Hard\", \"Insane\"\n",
    "        \"od\": float(get(\"OverallDifficulty\") or 0),  # timing precision required\n",
    "        \"ar\": float(get(\"ApproachRate\") or 0),        # how fast objects appear\n",
    "        \"cs\": float(get(\"CircleSize\") or 0),          # click target size\n",
    "        \"audio_path\": get(\"AudioFilename\"),\n",
    "    }\n",
    "\n",
    "def select_representatives(registry):\n",
    "    \"\"\"One beatmap per mapper per song — pick the highest OD.\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for r in registry:\n",
    "        groups[(r[\"song_idx\"], r[\"creator\"])].append(r)\n",
    "    \n",
    "    return [max(beatmaps, key=lambda b: b[\"od\"]) for beatmaps in groups.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Encode to 9-dim Signals\n",
    "\n",
    "Each representative beatmap is converted to a `[9, T]` numpy array — 9 channels over `T` audio frames (~172 fps). This uses the [osu-dreamer](https://github.com/jaswon/osu-dreamer) library's encoding:\n",
    "\n",
    "```\n",
    "Audio file ──→ Spectrogram (T frames at ~172 fps)\n",
    "                    │\n",
    ".osu file  ──→ encode_beatmap(beatmap, frame_times) ──→ [9, T] array\n",
    "```\n",
    "\n",
    "For songs with multiple mappers, we ensure all encodings share the same `T` (trimming to the shortest) so they're directly comparable frame-by-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from osu_dreamer.osu.beatmap import Beatmap\n",
    "# from osu_dreamer.data.load_audio import load_audio, get_frame_times\n",
    "# from osu_dreamer.data.beatmap.encode import encode_beatmap\n",
    "\n",
    "def encode_representatives(representatives):\n",
    "    \"\"\"Encode each representative beatmap to a 9-dim temporal signal.\"\"\"\n",
    "    encodings = []\n",
    "    \n",
    "    for rep in representatives:\n",
    "        # Load audio → spectrogram (determines frame count T)\n",
    "        spec = load_audio(Path(rep[\"audio_path\"]))\n",
    "        frame_times = get_frame_times(spec.shape[1])\n",
    "        \n",
    "        # Parse .osu file and encode hit objects to 9-dim signal\n",
    "        beatmap = Beatmap(Path(rep[\"osu_path\"]))\n",
    "        encoded = encode_beatmap(beatmap, frame_times)  # shape: [9, T]\n",
    "        \n",
    "        # Each row is one dimension over time:\n",
    "        # encoded[0] = ONSET signal  (1 where a hit object starts, 0 elsewhere)\n",
    "        # encoded[7] = X position    (normalized 0-1 over the playfield)\n",
    "        # ... etc.\n",
    "        \n",
    "        encodings.append({\"data\": encoded, **rep})\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Track A — Cross-Mapper Comparison (9-dim)\n",
    "\n",
    "The core analysis. For every pair of mappers who mapped the same song, we compute Pearson correlation **per dimension**.\n",
    "\n",
    "If ONSET has r=0.58 across all mapper pairs, that means mappers consistently agree on **where** to place hit objects — this is perceptual. If X position has r=0.02, mappers place objects in completely different spatial patterns — this is stylistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_NAMES = [\"ONSET\", \"COMBO\", \"SLIDE\", \"SUSTAIN\", \"WHISTLE\", \"FINISH\", \"CLAP\", \"X\", \"Y\"]\n",
    "\n",
    "def pearson_corr(a, b):\n",
    "    \"\"\"Pearson correlation between two 1-D time series.\"\"\"\n",
    "    if len(a) < 2 or np.std(a) == 0 or np.std(b) == 0:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "def cross_mapper_analysis(encodings_by_song):\n",
    "    \"\"\"\n",
    "    Compare every cross-mapper pair on the same song.\n",
    "    Returns per-dimension Pearson correlations averaged across all pairs.\n",
    "    \"\"\"\n",
    "    results = {dim: [] for dim in DIM_NAMES}\n",
    "    \n",
    "    for song_idx, song_encodings in encodings_by_song.items():\n",
    "        # Get minimum length across all encodings for this song\n",
    "        min_T = min(enc[\"data\"].shape[1] for enc in song_encodings)\n",
    "        \n",
    "        # Compare all cross-mapper pairs\n",
    "        for i in range(len(song_encodings)):\n",
    "            for j in range(i + 1, len(song_encodings)):\n",
    "                a = song_encodings[i]\n",
    "                b = song_encodings[j]\n",
    "                \n",
    "                if a[\"creator\"] == b[\"creator\"]:\n",
    "                    continue  # Skip same-mapper pairs\n",
    "                \n",
    "                # Compare each dimension independently\n",
    "                for d, dim_name in enumerate(DIM_NAMES):\n",
    "                    r = pearson_corr(\n",
    "                        a[\"data\"][d, :min_T],\n",
    "                        b[\"data\"][d, :min_T]\n",
    "                    )\n",
    "                    results[dim_name].append(r)\n",
    "    \n",
    "    # Average across all pairs\n",
    "    summary = {}\n",
    "    for dim_name in DIM_NAMES:\n",
    "        if results[dim_name]:\n",
    "            summary[dim_name] = {\n",
    "                \"pearson_mean\": np.mean(results[dim_name]),\n",
    "                \"pearson_std\": np.std(results[dim_name]),\n",
    "                \"n_pairs\": len(results[dim_name]),\n",
    "            }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example output (from our 368-song dataset):\n",
    "# ONSET    r=0.58  [PERCEPTUAL]  ← mappers agree on timing\n",
    "# SLIDE    r=0.33  [PERCEPTUAL]  ← sliders placed similarly\n",
    "# CLAP     r=0.12  [MIXED]       ← some agreement on hitsounds\n",
    "# X        r=0.02  [STYLISTIC]   ← cursor patterns are personal\n",
    "# Y        r=0.01  [STYLISTIC]   ← same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Encode Through VAE (Track B Setup)\n",
    "\n",
    "We pass each 9-dim encoding through a pre-trained **WaveNet VAE** (from the [osu-dreamer](https://github.com/jaswon/osu-dreamer) project). This compresses the 9-dim signal into a 32-dim latent representation:\n",
    "\n",
    "```\n",
    "[9, T] ──→ VAE encoder ──→ [32, T/18]\n",
    "```\n",
    "\n",
    "The temporal resolution drops by ~18x (the model's downsampling factor). The question Track B asks: **do these learned dimensions also show cross-mapper agreement?** If a latent dimension has high Pearson r, the VAE has learned something perceptually meaningful — even though it was never explicitly trained on cross-mapper data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from osu_dreamer.latent_model.model import Model\n",
    "\n",
    "def encode_latent(encodings_9dim, checkpoint_path):\n",
    "    \"\"\"Pass 9-dim encodings through the VAE encoder → 32-dim latent.\"\"\"\n",
    "    model = Model.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Use Apple Silicon (MPS), CUDA, or CPU\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \\\n",
    "             \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    latent_encodings = []\n",
    "    for enc in encodings_9dim:\n",
    "        x = torch.tensor(enc[\"data\"]).float().unsqueeze(0).to(device)  # [1, 9, T]\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(x)  # [1, 32, T/18]\n",
    "        latent_encodings.append({\"data\": z.squeeze(0).cpu().numpy(), **enc})\n",
    "    \n",
    "    return latent_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Track B — Cross-Mapper Comparison (32-dim latent)\n",
    "\n",
    "Same methodology as Track A, but on the 32-dim latent space. We compute Pearson r for each of the 32 latent dimensions across all cross-mapper pairs.\n",
    "\n",
    "The code is identical to `cross_mapper_analysis()` above, just with 32 dimensions instead of 9 and the latent encodings as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LATENT = 32\n",
    "\n",
    "# Track B uses the exact same cross-mapper comparison as Track A,\n",
    "# just operating on the 32-dim latent encodings instead of 9-dim.\n",
    "#\n",
    "# The output classifies each latent dimension:\n",
    "#   z_12  r=0.41  [PERCEPTUAL]  ← this latent dim captures something universal\n",
    "#   z_05  r=0.22  [MIXED]\n",
    "#   z_28  r=0.01  [STYLISTIC]   ← this dim captures mapper-specific style\n",
    "#\n",
    "# Key finding: some VAE dimensions DO show cross-mapper agreement,\n",
    "# meaning the model learned perceptually meaningful features from data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Synthesis — Connecting the Two Spaces\n",
    "\n",
    "Finally, we build a **32 x 9 correlation matrix** to understand what each latent dimension encodes in terms of the interpretable dimensions.\n",
    "\n",
    "For each beatmap that has both encodings, we:\n",
    "1. Downsample the 9-dim signal to match the latent temporal resolution (average 18-frame blocks)\n",
    "2. Compute Pearson r between each latent dim and each interpretable dim\n",
    "3. Average across all beatmaps\n",
    "\n",
    "This tells us: if latent dimension z_12 is perceptual (high cross-mapper r), **what** does it encode? Maybe it correlates strongly with ONSET — meaning the VAE independently learned to represent hit timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_correlation_matrix(encodings_9dim, encodings_latent):\n",
    "    \"\"\"\n",
    "    Build a [32 x 9] matrix: correlation between each latent dim\n",
    "    and each interpretable dim, averaged across all beatmaps.\n",
    "    \"\"\"\n",
    "    corr_matrix = np.zeros((N_LATENT, len(DIM_NAMES)))\n",
    "    count = 0\n",
    "    \n",
    "    for enc_9, enc_z in zip(encodings_9dim, encodings_latent):\n",
    "        T_latent = enc_z[\"data\"].shape[1]\n",
    "        \n",
    "        # Downsample 9-dim to match latent temporal resolution\n",
    "        # The VAE's encoder downsamples by ~18x\n",
    "        enc_9_downsampled = np.zeros((9, T_latent))\n",
    "        for dim in range(9):\n",
    "            for t in range(T_latent):\n",
    "                start = t * 18\n",
    "                end = min(start + 18, enc_9[\"data\"].shape[1])\n",
    "                enc_9_downsampled[dim, t] = enc_9[\"data\"][dim, start:end].mean()\n",
    "        \n",
    "        # Correlate each latent dim with each interpretable dim\n",
    "        for z_dim in range(N_LATENT):\n",
    "            for i_dim in range(9):\n",
    "                z_signal = enc_z[\"data\"][z_dim]\n",
    "                i_signal = enc_9_downsampled[i_dim]\n",
    "                if np.std(z_signal) > 0 and np.std(i_signal) > 0:\n",
    "                    r = np.corrcoef(z_signal, i_signal)[0, 1]\n",
    "                    if not np.isnan(r):\n",
    "                        corr_matrix[z_dim, i_dim] += r\n",
    "        count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        corr_matrix /= count\n",
    "    \n",
    "    return corr_matrix  # [32, 9] — rows are latent dims, columns are interpretable dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "The full pipeline runs these steps in sequence:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  .osz files (beatmap archives)                      │\n",
    "└──────────────────────┬──────────────────────────────┘\n",
    "                       │\n",
    "            Step 1: Find multi-mapper songs\n",
    "            Step 2: Parse metadata, select representatives\n",
    "                       │\n",
    "            Step 3: Encode to 9-dim signals\n",
    "                       │\n",
    "              ┌────────┴────────┐\n",
    "              │                 │\n",
    "     Step 4: Track A      Step 5: VAE encode\n",
    "     (9-dim Pearson r)    (9-dim → 32-dim latent)\n",
    "              │                 │\n",
    "              │            Step 6: Track B\n",
    "              │            (32-dim Pearson r)\n",
    "              │                 │\n",
    "              └────────┬────────┘\n",
    "                       │\n",
    "            Step 7: Synthesis\n",
    "            (32 x 9 correlation matrix)\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Track B is optional — if no VAE checkpoint is provided, only Track A runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running It\n",
    "\n",
    "**GUI** (recommended — interactive results viewer):\n",
    "```bash\n",
    "git clone -b core-pipeline https://github.com/recxa/osu-dreamer-cartography.git\n",
    "cd osu-dreamer-cartography\n",
    "python3 -m venv .venv && source .venv/bin/activate\n",
    "pip install -e .\n",
    "python run_gui.py\n",
    "```\n",
    "\n",
    "**CLI** (headless, for scripting):\n",
    "```bash\n",
    "python experiment/run_analysis.py /path/to/your/osz/files/\n",
    "```\n",
    "\n",
    "Results are saved to `experiment/output/results/` as JSON + PNG plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Pipeline code: [recxa/osu-dreamer-cartography](https://github.com/recxa/osu-dreamer-cartography) (core-pipeline branch)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}